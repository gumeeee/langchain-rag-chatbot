{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgaHx8AyZ6tP"
      },
      "source": [
        "# Cadeias de Conversa√ß√£o Robusta com LangChain e Google Gemini\n",
        "\n",
        "## Notebook Pr√°tico - Aula 5\n",
        "\n",
        "**Instrutor:** Leonardo Pena  \n",
        "**Curso:** LangChain: Criando chatbots inteligentes com RAG\n",
        "\n",
        "---\n",
        "\n",
        "Este notebook apresenta implementa√ß√µes pr√°ticas dos conceitos abordados na aula sobre **Cadeias de Conversa√ß√£o Robusta**, utilizando o **Google Gemini** como modelo de linguagem, incluindo:\n",
        "\n",
        "1. **ConversationalRetrievalChain** - Gerenciamento de contexto conversacional\n",
        "2. **Gerenciamento de Mem√≥ria** - T√©cnicas para manter hist√≥rico de conversas\n",
        "3. **Guardrails de Seguran√ßa** - Filtros para respostas seguras\n",
        "4. **Re-ranking** - Melhoria na qualidade da recupera√ß√£o\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JCAvkqlZ6tS"
      },
      "source": [
        "## üì¶ Instala√ß√£o e Importa√ß√µes\n",
        "\n",
        "Primeiro, vamos instalar as depend√™ncias necess√°rias e importar as bibliotecas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apn0v-i3Z6tS",
        "outputId": "2721cff5-d42f-4ed2-fcd5-e79f9c2014b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.3.26\n",
            "  Using cached langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain-community==0.3.27\n",
            "  Using cached langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-core==0.3.71\n",
            "  Using cached langchain_core-0.3.71-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting langchain-google-genai==2.1.8\n",
            "  Using cached langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain==0.3.26)\n",
            "  Using cached langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.26) (0.4.59)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.26) (2.12.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.26) (2.0.45)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.26) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.26) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.3.27) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core==0.3.71) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core==0.3.71) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core==0.3.71) (25.0)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai==2.1.8) (1.2.0)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai==2.1.8)\n",
            "  Using cached google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.3)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.39.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.5)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "INFO: pip is looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-generativeai\n",
            "  Using cached google_generativeai-0.8.6-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Using cached google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.8.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "INFO: pip is still looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.7.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.6.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached google_generativeai-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Using cached google_generativeai-0.4.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Using cached google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached google_generativeai-0.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached google_generativeai-0.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached google_generativeai-0.2.2-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Using cached google_generativeai-0.2.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Using cached google_generativeai-0.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Using cached google_generativeai-0.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "\u001b[31mERROR: Cannot install google-generativeai==0.1.0, google-generativeai==0.2.0, google-generativeai==0.2.1, google-generativeai==0.2.2, google-generativeai==0.3.0, google-generativeai==0.3.1, google-generativeai==0.3.2, google-generativeai==0.4.0, google-generativeai==0.4.1, google-generativeai==0.5.0, google-generativeai==0.5.1, google-generativeai==0.5.2, google-generativeai==0.5.3, google-generativeai==0.5.4, google-generativeai==0.6.0, google-generativeai==0.7.0, google-generativeai==0.7.1, google-generativeai==0.7.2, google-generativeai==0.8.0, google-generativeai==0.8.1, google-generativeai==0.8.2, google-generativeai==0.8.3, google-generativeai==0.8.4, google-generativeai==0.8.5, google-generativeai==0.8.6 and langchain-google-genai==2.1.8 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.8.5 depends on google-ai-generativelanguage==0.6.15\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.8.6 depends on google-ai-generativelanguage==0.6.15\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.8.4 depends on google-ai-generativelanguage==0.6.15\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.8.3 depends on google-ai-generativelanguage==0.6.10\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.8.2 depends on google-ai-generativelanguage==0.6.10\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.8.1 depends on google-ai-generativelanguage==0.6.9\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.8.0 depends on google-ai-generativelanguage==0.6.9\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.7.2 depends on google-ai-generativelanguage==0.6.6\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.7.1 depends on google-ai-generativelanguage==0.6.6\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.7.0 depends on google-ai-generativelanguage==0.6.5\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.6.0 depends on google-ai-generativelanguage==0.6.4\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.5.4 depends on google-ai-generativelanguage==0.6.4\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.5.3 depends on google-ai-generativelanguage==0.6.3\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.5.2 depends on google-ai-generativelanguage==0.6.2\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.5.1 depends on google-ai-generativelanguage==0.6.2\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.5.0 depends on google-ai-generativelanguage==0.6.1\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.4.1 depends on google-ai-generativelanguage==0.4.0\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.4.0 depends on google-ai-generativelanguage==0.4.0\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.3.2 depends on google-ai-generativelanguage==0.4.0\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.3.1 depends on google-ai-generativelanguage==0.4.0\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.3.0 depends on google-ai-generativelanguage==0.4.0\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.2.2 depends on google-ai-generativelanguage==0.3.3\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.2.1 depends on google-ai-generativelanguage==0.3.3\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.2.0 depends on google-ai-generativelanguage==0.3.2\n",
            "    langchain-google-genai 2.1.8 depends on google-ai-generativelanguage<0.7.0 and >=0.6.18\n",
            "    google-generativeai 0.1.0 depends on google-ai-generativelanguage==0.2.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-google-genai langchain-community chromadb tiktoken google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qYbt_D46Z6tT",
        "outputId": "7c608cb9-0480-4934-9b40-2bf01413f71a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain.chains'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2892759139.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Importa√ß√µes necess√°rias                                   # t√≠tulo da se√ß√£o\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversationalRetrievalChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversationBufferWindowMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_chroma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.chains'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Importa√ß√µes necess√°rias                                   # t√≠tulo da se√ß√£o\n",
        "import os\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "import google.generativeai as genai\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRAbmDgQZ6tT"
      },
      "source": [
        "## üîß Configura√ß√£o Inicial\n",
        "\n",
        "Vamos configurar a chave de API do Google Gemini e inicializar os componentes b√°sicos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMpnjrldZ6tU"
      },
      "outputs": [],
      "source": [
        "# Configura√ß√£o da chave de API do Google Gemini\n",
        "# Voc√™ pode obter sua chave em: https://makersuite.google.com/app/apikey\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = 'AIzaSyBDc4dXCuYxVb9bXWERnXNX95qmHtXaVxg'  # Define a vari√°vel de ambiente com a chave da API do Google Gemini (n√£o compartilhe essa chave publicamente)\n",
        "GOOGLE_API_KEY = os.environ[\"GOOGLE_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoDoz-RWZ6tU"
      },
      "source": [
        "## üìö 1. Prepara√ß√£o dos Dados\n",
        "\n",
        "Vamos criar alguns documentos de exemplo para demonstrar os conceitos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvtv-6cZZ6tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4876a4c-ca64-481a-be16-6716a5114b27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Criados 6 documentos de exemplo\n"
          ]
        }
      ],
      "source": [
        "# Cria√ß√£o de documentos de exemplo sobre IA e Machine Learning\n",
        "documentos_exemplo = [\n",
        "    \"\"\"Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia da computa√ß√£o que se concentra\n",
        "    na cria√ß√£o de sistemas capazes de realizar tarefas que normalmente requerem intelig√™ncia humana.\n",
        "    Isso inclui aprendizado, racioc√≠nio, percep√ß√£o e tomada de decis√µes.\"\"\",\n",
        "\n",
        "    \"\"\"Machine Learning √© uma sub√°rea da IA que permite que computadores aprendam e melhorem\n",
        "    automaticamente atrav√©s da experi√™ncia, sem serem explicitamente programados.\n",
        "    Os algoritmos de ML identificam padr√µes em dados e fazem previs√µes.\"\"\",\n",
        "\n",
        "    \"\"\"Deep Learning √© uma t√©cnica de machine learning baseada em redes neurais artificiais\n",
        "    com m√∫ltiplas camadas. √â especialmente eficaz para tarefas como reconhecimento de imagem,\n",
        "    processamento de linguagem natural e reconhecimento de voz.\"\"\",\n",
        "\n",
        "    \"\"\"RAG (Retrieval-Augmented Generation) √© uma t√©cnica que combina recupera√ß√£o de informa√ß√µes\n",
        "    com gera√ß√£o de texto. Permite que modelos de linguagem acessem conhecimento externo\n",
        "    para gerar respostas mais precisas e atualizadas.\"\"\",\n",
        "\n",
        "    \"\"\"LangChain √© um framework para desenvolvimento de aplica√ß√µes com modelos de linguagem.\n",
        "    Facilita a cria√ß√£o de cadeias complexas, gerenciamento de mem√≥ria e integra√ß√£o\n",
        "    com diferentes fontes de dados.\"\"\",\n",
        "\n",
        "    \"\"\"Google Gemini √© um modelo de linguagem multimodal desenvolvido pelo Google,\n",
        "    capaz de processar texto, imagens e c√≥digo. Oferece capacidades avan√ßadas de\n",
        "    racioc√≠nio e compreens√£o contextual.\"\"\"\n",
        "]\n",
        "\n",
        "# Convers√£o para objetos Document\n",
        "docs = [Document(page_content=doc) for doc in documentos_exemplo]\n",
        "\n",
        "print(f\"‚úÖ Criados {len(docs)} documentos de exemplo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scnWKr5qZ6tU"
      },
      "source": [
        "## üîç 2. Cria√ß√£o do Vector Store com Gemini Embeddings\n",
        "\n",
        "Vamos criar um vector store usando Chroma com embeddings do Google Gemini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8DkTkNkZ6tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0c19de-171b-4cb3-edda-4680a83a80a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N√∫mero de documentos indexados: 6\n"
          ]
        }
      ],
      "source": [
        "# Inicializacao dos embeddings do Google Gemini\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Cria√ßao do vector store\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding= embeddings,\n",
        "    persist_directory=\"./chroma_db_gemini\")\n",
        "\n",
        "print(f\"N√∫mero de documentos indexados: {vectorstore._collection.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X0jj70AZ6tV"
      },
      "source": [
        "## üß† 3. Gerenciamento de Mem√≥ria\n",
        "\n",
        "Implementa√ß√£o do **ConversationBufferWindowMemory** para manter contexto das conversas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhqFKOpGZ6tV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba0bf70-367a-4fbc-a663-4b6619bb3a1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Mem√≥ria configurada!\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "memory = ConversationBufferWindowMemory(\n",
        "    k=5,\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    output_key=\"answer\"\n",
        ")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Mem√≥ria configurada!\")\n",
        "print(memory.k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekeVBNkcZ6tV"
      },
      "source": [
        "## üîó 4. ConversationalRetrievalChain com Google Gemini\n",
        "\n",
        "Cria√ß√£o da cadeia conversacional que combina recupera√ß√£o de documentos com o modelo Gemini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wuDhnq4Z6tV"
      },
      "outputs": [],
      "source": [
        "# Inicializa√ßao do modelo Google Gemini\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro-latest\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7,\n",
        "    convert_system_message_to_human=True\n",
        ")\n",
        "\n",
        "\n",
        "# Cria√ßao da ConversationalRetrievalChain\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    memory=memory,\n",
        "    return_source_documents=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-f1uZNSZ6tV"
      },
      "source": [
        "## üí¨ 5. Testando a Conversa√ß√£o\n",
        "\n",
        "Vamos testar nossa cadeia conversacional com algumas perguntas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKzHY_GuZ6tV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6892b36-4b9a-466c-fa55-8b1513c33fba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Pergunta: O que √© Intelig√™ncia Artificial?\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia da computa√ß√£o que se concentra\n",
            "    na cria√ß√£o de sistemas capazes de realizar tarefas que normalmente requerem intelig√™ncia humana.\n",
            "    Isso inclui aprendizado, racioc√≠nio, percep√ß√£o e tomada de decis√µes.\n",
            "\n",
            "Machine Learning √© uma sub√°rea da IA que permite que computadores aprendam e melhorem\n",
            "    automaticamente atrav√©s da experi√™ncia, sem serem explicitamente programados.\n",
            "    Os algoritmos de ML identificam padr√µes em dados e fazem previs√µes.\n",
            "\n",
            "Deep Learning √© uma t√©cnica de machine learning baseada em redes neurais artificiais\n",
            "    com m√∫ltiplas camadas. √â especialmente eficaz para tarefas como reconhecimento de imagem,\n",
            "    processamento de linguagem natural e reconhecimento de voz.\n",
            "Human: O que √© Intelig√™ncia Artificial?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "ü§ñ Resposta: Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia da computa√ß√£o que se concentra na cria√ß√£o de sistemas capazes de realizar tarefas que normalmente requerem intelig√™ncia humana. Isso inclui aprendizado, racioc√≠nio, percep√ß√£o e tomada de decis√µes.\n",
            "\n",
            "üìö Documentos utilizados: 3\n"
          ]
        }
      ],
      "source": [
        "def fazer_pergunta(pergunta):\n",
        "    \"\"\"Fun√ß√£o auxiliar para fazer perguntas √† cadeia conversacional\"\"\"\n",
        "\n",
        "    print(f\"\\n Pergunta: {pergunta}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    try:\n",
        "        resultado = qa_chain({\"question\": pergunta})\n",
        "\n",
        "        print(f\"ü§ñ Resposta: {resultado['answer']}\")\n",
        "        print(f\"\\nüìö Documentos utilizados: {len(resultado['source_documents'])}\")\n",
        "\n",
        "        return resultado\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Primeira pergunta\n",
        "resultado1 = fazer_pergunta(\"O que √© Intelig√™ncia Artificial?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jBJrH6nZ6tV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c81b89-9ced-4a7f-e9f4-c5ab71208b4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Pergunta: Como ela se relaciona com Machine Learning\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: O que √© Intelig√™ncia Artificial?\n",
            "Assistant: Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia da computa√ß√£o que se concentra na cria√ß√£o de sistemas capazes de realizar tarefas que normalmente requerem intelig√™ncia humana. Isso inclui aprendizado, racioc√≠nio, percep√ß√£o e tomada de decis√µes.\n",
            "Follow Up Input: Como ela se relaciona com Machine Learning\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "Machine Learning √© uma sub√°rea da IA que permite que computadores aprendam e melhorem\n",
            "    automaticamente atrav√©s da experi√™ncia, sem serem explicitamente programados.\n",
            "    Os algoritmos de ML identificam padr√µes em dados e fazem previs√µes.\n",
            "\n",
            "Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia da computa√ß√£o que se concentra\n",
            "    na cria√ß√£o de sistemas capazes de realizar tarefas que normalmente requerem intelig√™ncia humana.\n",
            "    Isso inclui aprendizado, racioc√≠nio, percep√ß√£o e tomada de decis√µes.\n",
            "\n",
            "Deep Learning √© uma t√©cnica de machine learning baseada em redes neurais artificiais\n",
            "    com m√∫ltiplas camadas. √â especialmente eficaz para tarefas como reconhecimento de imagem,\n",
            "    processamento de linguagem natural e reconhecimento de voz.\n",
            "Human: Qual a rela√ß√£o entre Intelig√™ncia Artificial e Machine Learning?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "ü§ñ Resposta: Machine learning (ML) √© uma sub√°rea da Intelig√™ncia Artificial (IA).\n",
            "\n",
            "üìö Documentos utilizados: 3\n"
          ]
        }
      ],
      "source": [
        "resultado2 = fazer_pergunta(\"Como ela se relaciona com Machine Learning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSSMymmTZ6tW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e0957bc-57ad-47ad-bd4d-5a7b70c1ccd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Pergunta: E o que √© Google Gemini como voce mencionou?\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: O que √© Intelig√™ncia Artificial?\n",
            "Assistant: Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia da computa√ß√£o que se concentra na cria√ß√£o de sistemas capazes de realizar tarefas que normalmente requerem intelig√™ncia humana. Isso inclui aprendizado, racioc√≠nio, percep√ß√£o e tomada de decis√µes.\n",
            "Human: Como ela se relaciona com Machine Learning\n",
            "Assistant: Machine learning (ML) √© uma sub√°rea da Intelig√™ncia Artificial (IA).\n",
            "Follow Up Input: E o que √© Google Gemini como voce mencionou?\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "Google Gemini √© um modelo de linguagem multimodal desenvolvido pelo Google,\n",
            "    capaz de processar texto, imagens e c√≥digo. Oferece capacidades avan√ßadas de\n",
            "    racioc√≠nio e compreens√£o contextual.\n",
            "\n",
            "Machine Learning √© uma sub√°rea da IA que permite que computadores aprendam e melhorem\n",
            "    automaticamente atrav√©s da experi√™ncia, sem serem explicitamente programados.\n",
            "    Os algoritmos de ML identificam padr√µes em dados e fazem previs√µes.\n",
            "\n",
            "Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia da computa√ß√£o que se concentra\n",
            "    na cria√ß√£o de sistemas capazes de realizar tarefas que normalmente requerem intelig√™ncia humana.\n",
            "    Isso inclui aprendizado, racioc√≠nio, percep√ß√£o e tomada de decis√µes.\n",
            "Human: O que √© Google Gemini?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "ü§ñ Resposta: Google Gemini √© um modelo de linguagem multimodal desenvolvido pelo Google, capaz de processar texto, imagens e c√≥digo. Oferece capacidades avan√ßadas de racioc√≠nio e compreens√£o contextual.\n",
            "\n",
            "üìö Documentos utilizados: 3\n"
          ]
        }
      ],
      "source": [
        "resultado3 = fazer_pergunta(\"E o que √© Google Gemini como voce mencionou?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhjcdmvQZ6tW"
      },
      "source": [
        "## üõ°Ô∏è 6. Guardrails de Seguran√ßa\n",
        "\n",
        "Implementa√ß√£o de filtros de seguran√ßa para prevenir respostas inadequadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3qr1H5hZ6tW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "612c84b4-d56c-4710-9d14-ec9c8d03cf91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Guardrails de seguran√ßa configurados!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "class GuardrailsSeguranca:\n",
        "    def __init__(self):\n",
        "        self.palavras_proibidas = [\n",
        "            'senha', 'password', 'cpf', 'rg', 'cart√£o de cr√©dito',\n",
        "            'dados pessoais', 'informa√ß√£o confidencial', 'api key',\n",
        "            'chave de api', 'token de acesso'\n",
        "        ]\n",
        "\n",
        "        self.padroes_pii = [\n",
        "            r'\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2}',        # Padr√£o CPF (999.999.999-99)\n",
        "            r'\\d{4}\\s?\\d{4}\\s?\\d{4}\\s?\\d{4}',    # Padr√£o cart√£o de cr√©dito (16 d√≠gitos)\n",
        "            r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',  # Padr√£o e-mail\n",
        "            r'AIza[0-9A-Za-z\\-_]{35}'            # Padr√£o Google API Key\n",
        "        ]\n",
        "\n",
        "    def verificar_pergunta(self, pergunta):\n",
        "        \"\"\"Verifica se a pergunta cont√©m conte√∫do inadequado\"\"\"\n",
        "        pergunta_lower = pergunta.lower()\n",
        "\n",
        "        # Verificar palavras proibidas\n",
        "        for palavra in self.palavras_proibidas:\n",
        "            if palavra in pergunta_lower:\n",
        "                return False, f\"Pergunta cont√©m termo inadequado: {palavra}\"\n",
        "\n",
        "        # Verificar padr√µes PII\n",
        "        for padrao in self.padroes_pii:\n",
        "            if re.search(padrao, pergunta):\n",
        "                return False, \"Pergunta cont√©m informa√ß√µes pessoais\"\n",
        "\n",
        "        return True, \"Pergunta aprovada\"\n",
        "\n",
        "    def verificar_resposta(self, resposta):\n",
        "        \"\"\"Verifica se a resposta √© adequada\"\"\"\n",
        "        resposta_lower = resposta.lower()\n",
        "\n",
        "        # Verificar se a resposta est√° no escopo\n",
        "        termos_escopo = ['ia', 'intelig√™ncia artificial', 'machine learning',\n",
        "                         'deep learning', 'rag', 'langchain', 'gemini', 'google']  # T√≥picos permitidos\n",
        "\n",
        "        tem_termo_escopo = any(termo in resposta_lower for termo in termos_escopo)  # Confere se la tem pelo menos um termo relevante\n",
        "\n",
        "        if not tem_termo_escopo and len(resposta) > 50:  # Se n√£o for do escopo e ainda for longa\n",
        "            return False, \"Resposta fora do escopo da aplica√ß√£o\"  # Bloqueia\n",
        "\n",
        "        # Verificar se n√£o cont√©m informa√ß√µes sens√≠veis\n",
        "        for padrao in self.padroes_pii:                 # Percorre cada regex de PII\n",
        "            if re.search(padrao, resposta):             # Se encontrar dado sens√≠vel\n",
        "                return False, \"Resposta cont√©m informa√ß√µes sens√≠veis\"  # Bloqueia\n",
        "\n",
        "        return True, \"Resposta aprovada\"  # Retorna sucesso se tudo estiver OK\n",
        "\n",
        "# Inicializa√ß√£o dos guardrails\n",
        "guardrails = GuardrailsSeguranca()  # Cria a inst√¢ncia dos guardrails\n",
        "print(\"‚úÖ Guardrails de seguran√ßa configurados!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkNHisHmZ6tW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d7536f5-3c84-4d41-d7b5-4ab66d104e57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Teste com pergunta adequada ===\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
            "\n",
            "Chat History:\n",
            "\n",
            "Human: O que √© Intelig√™ncia Artificial?\n",
            "Assistant: Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia da computa√ß√£o que se concentra na cria√ß√£o de sistemas capazes de realizar tarefas que normalmente requerem intelig√™ncia humana. Isso inclui aprendizado, racioc√≠nio, percep√ß√£o e tomada de decis√µes.\n",
            "Human: Como ela se relaciona com Machine Learning\n",
            "Assistant: Machine learning (ML) √© uma sub√°rea da Intelig√™ncia Artificial (IA).\n",
            "Human: E o que √© Google Gemini como voce mencionou?\n",
            "Assistant: Google Gemini √© um modelo de linguagem multimodal desenvolvido pelo Google, capaz de processar texto, imagens e c√≥digo. Oferece capacidades avan√ßadas de racioc√≠nio e compreens√£o contextual.\n",
            "Follow Up Input: Explique sobre Deep Learning\n",
            "Standalone question:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "Deep Learning √© uma t√©cnica de machine learning baseada em redes neurais artificiais\n",
            "    com m√∫ltiplas camadas. √â especialmente eficaz para tarefas como reconhecimento de imagem,\n",
            "    processamento de linguagem natural e reconhecimento de voz.\n",
            "\n",
            "Machine Learning √© uma sub√°rea da IA que permite que computadores aprendam e melhorem\n",
            "    automaticamente atrav√©s da experi√™ncia, sem serem explicitamente programados.\n",
            "    Os algoritmos de ML identificam padr√µes em dados e fazem previs√µes.\n",
            "\n",
            "Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia da computa√ß√£o que se concentra\n",
            "    na cria√ß√£o de sistemas capazes de realizar tarefas que normalmente requerem intelig√™ncia humana.\n",
            "    Isso inclui aprendizado, racioc√≠nio, percep√ß√£o e tomada de decis√µes.\n",
            "Human: O que √© Deep Learning?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "‚úÖ Pergunta aprovada\n",
            "‚úÖ Resposta aprovada\n",
            "\n",
            "ü§ñ Resposta: Deep Learning √© uma t√©cnica de machine learning baseada em redes neurais artificiais com m√∫ltiplas camadas. √â especialmente eficaz para tarefas como reconhecimento de imagem, processamento de linguagem natural e reconhecimento de voz.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Explique sobre Deep Learning',\n",
              " 'chat_history': [HumanMessage(content='O que √© Intelig√™ncia Artificial?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia da computa√ß√£o que se concentra na cria√ß√£o de sistemas capazes de realizar tarefas que normalmente requerem intelig√™ncia humana. Isso inclui aprendizado, racioc√≠nio, percep√ß√£o e tomada de decis√µes.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Como ela se relaciona com Machine Learning', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Machine learning (ML) √© uma sub√°rea da Intelig√™ncia Artificial (IA).', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='E o que √© Google Gemini como voce mencionou?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Google Gemini √© um modelo de linguagem multimodal desenvolvido pelo Google, capaz de processar texto, imagens e c√≥digo. Oferece capacidades avan√ßadas de racioc√≠nio e compreens√£o contextual.', additional_kwargs={}, response_metadata={})],\n",
              " 'answer': 'Deep Learning √© uma t√©cnica de machine learning baseada em redes neurais artificiais com m√∫ltiplas camadas. √â especialmente eficaz para tarefas como reconhecimento de imagem, processamento de linguagem natural e reconhecimento de voz.',\n",
              " 'source_documents': [Document(metadata={}, page_content='Deep Learning √© uma t√©cnica de machine learning baseada em redes neurais artificiais\\n    com m√∫ltiplas camadas. √â especialmente eficaz para tarefas como reconhecimento de imagem,\\n    processamento de linguagem natural e reconhecimento de voz.'),\n",
              "  Document(metadata={}, page_content='Machine Learning √© uma sub√°rea da IA que permite que computadores aprendam e melhorem\\n    automaticamente atrav√©s da experi√™ncia, sem serem explicitamente programados.\\n    Os algoritmos de ML identificam padr√µes em dados e fazem previs√µes.'),\n",
              "  Document(metadata={}, page_content='Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia da computa√ß√£o que se concentra\\n    na cria√ß√£o de sistemas capazes de realizar tarefas que normalmente requerem intelig√™ncia humana.\\n    Isso inclui aprendizado, racioc√≠nio, percep√ß√£o e tomada de decis√µes.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "def pergunta_segura(pergunta):\n",
        "    \"\"\"Fun√ß√£o que aplica guardrails antes de processar a pergunta\"\"\"\n",
        "    aprovada, mensagem = guardrails.verificar_pergunta(pergunta)\n",
        "\n",
        "    if not aprovada:\n",
        "        print(f\"üö´ Pergunta rejeitada: {mensagem}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Processar pergunta\n",
        "        resultado = qa_chain({\"question\": pergunta})\n",
        "\n",
        "        # Verificar resposta\n",
        "        aprovada_resp, mensagem_resp = guardrails.verificar_resposta(\n",
        "            resultado['answer']\n",
        "        )\n",
        "\n",
        "        if not aprovada_resp:\n",
        "            print(f\"üö´ Resposta rejeitada: {mensagem_resp}\")\n",
        "            return None\n",
        "\n",
        "        print(f\"‚úÖ {mensagem}\")\n",
        "        print(f\"‚úÖ {mensagem_resp}\")\n",
        "        print(f\"\\nü§ñ Resposta: {resultado['answer']}\")\n",
        "\n",
        "        return resultado\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao processar pergunta: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Teste com pergunta adequada\n",
        "print(\"=== Teste com pergunta adequada ===\")\n",
        "pergunta_segura(\"Explique sobre Deep Learning\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNAJexp8Z6tW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5afb48f6-0e17-4774-bc76-3e63e01a1df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Teste com pergunta inadequada ===\n",
            "üö´ Pergunta rejeitada: Pergunta cont√©m termo inadequado: chave de api\n"
          ]
        }
      ],
      "source": [
        "# Teste com pergunta inadequada\n",
        "print(\"\\n=== Teste com pergunta inadequada ===\")\n",
        "pergunta_segura(\"Qual √© a sua chave de API?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB7jh0zUZ6tW"
      },
      "source": [
        "## üîÑ 7. Re-ranking Melhorado\n",
        "\n",
        "Implementa√ß√£o de re-ranking para melhorar a qualidade da recupera√ß√£o de documentos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7-qiF6aZ6tW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5ebfb3-5e08-4ea3-c0d0-a5ba4b9d6a0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Re-ranker com Gemini configurado!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np  # Importa NumPy, embora n√£o seja usado diretamente aqui (poderia ser removido)\n",
        "from sklearn.metrics.pairwise import cosine_similarity  # Fun√ß√£o para calcular similaridade cosseno entre vetores\n",
        "\n",
        "class RerankGemini:  # Classe respons√°vel por reordenar (re-rankear) documentos com base em embeddings do Gemini\n",
        "    def __init__(self, embeddings_model):  # Construtor que recebe um modelo de embeddings\n",
        "        self.embeddings_model = embeddings_model  # Armazena o embedder fornecido\n",
        "        self.nome = \"Re-ranking com Gemini Embeddings\"  # Nome descritivo para identifica√ß√£o\n",
        "\n",
        "    def rerank(self, query, documents, top_k=3):  # M√©todo principal de re-ranking; devolve top_k docs mais relevantes\n",
        "        \"\"\"Re-ranking baseado em similaridade sem√¢ntica usando Gemini embeddings\"\"\"  # Docstring explicativa\n",
        "        try:  # Tenta executar o fluxo principal (pode falhar, ent√£o h√° fallback)\n",
        "            # Gerar embedding da query\n",
        "            query_embedding = self.embeddings_model.embed_query(query)  # Cria vetor da pergunta usando o modelo de embeddings\n",
        "\n",
        "            # Gerar embeddings dos documentos\n",
        "            doc_texts = [doc.page_content if hasattr(doc, 'page_content') else str(doc) for doc in documents]  # Extrai texto de cada Document\n",
        "            doc_embeddings = self.embeddings_model.embed_documents(doc_texts)  # Converte textos em vetores\n",
        "\n",
        "            # Calcular similaridades\n",
        "            similarities = cosine_similarity([query_embedding], doc_embeddings)[0]  # Calcula similaridade cosseno entre query e cada doc\n",
        "\n",
        "            # Criar lista de documentos com scores\n",
        "            scored_docs = list(zip(similarities, documents))  # Combina score e doc em tuplas\n",
        "\n",
        "            # Ordenar por similaridade (maior primeiro)\n",
        "            scored_docs.sort(key=lambda x: x[0], reverse=True)  # Ordena pelo score decrescente\n",
        "\n",
        "            # Retornar top_k documentos\n",
        "            return [doc for _, doc in scored_docs[:top_k]]  # Retorna somente os documentos mais relevantes\n",
        "\n",
        "        except Exception as e:  # Captura exce√ß√µes (ex.: falha na API)\n",
        "            print(f\"Erro no re-ranking: {e}\")  # Exibe mensagem de erro\n",
        "            # Fallback para re-ranking simples\n",
        "            return self._simple_rerank(query, documents, top_k)  # Usa m√©todo secund√°rio caso ocorra erro\n",
        "\n",
        "    def _simple_rerank(self, query, documents, top_k):  # M√©todo de fallback para re-ranking por interse√ß√£o de palavras\n",
        "        \"\"\"Fallback: re-ranking simples baseado em palavras-chave\"\"\"  # Docstring\n",
        "        query_words = set(query.lower().split())  # Divide a query em palavras (lowercase) para compara√ß√£o\n",
        "\n",
        "        scored_docs = []  # Lista para (score, doc)\n",
        "        for doc in documents:  # Itera sobre documentos\n",
        "            doc_text = doc.page_content if hasattr(doc, 'page_content') else str(doc)  # Obt√©m texto do doc\n",
        "            doc_words = set(doc_text.lower().split())  # Converte texto em conjunto de palavras\n",
        "            score = len(query_words.intersection(doc_words)) / len(query_words) if query_words else 0  # Percentual de palavras em comum\n",
        "            scored_docs.append((score, doc))  # Adiciona tupla (score, doc) √† lista\n",
        "\n",
        "        scored_docs.sort(key=lambda x: x[0], reverse=True)  # Ordena pelo score de interse√ß√£o\n",
        "        return [doc for _, doc in scored_docs[:top_k]]  # Retorna top_k docs ap√≥s ordena√ß√£o\n",
        "\n",
        "# Inicializa√ß√£o do re-ranker com Gemini\n",
        "reranker = RerankGemini(embeddings)  # Cria inst√¢ncia passando o embedder Gemini previamente configurado\n",
        "print(\"‚úÖ Re-ranker com Gemini configurado!\")  # Mensagem de sucesso ao criar o re-ranker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lLjglONZ6tX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04be96d-4d98-4efd-eb0f-2edfc9d2f7cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Buscando documentos para: 'machine learning algoritmos gemini'\n",
            "üìÑ Documentos encontrados na busca inicial: 5\n",
            "üéØ Documentos ap√≥s re-ranking: 3\n",
            "\n",
            "üìä Documentos selecionados ap√≥s re-ranking:\n",
            "1. Google Gemini √© um modelo de linguagem multimodal desenvolvido pelo Google,\n",
            "    capaz de processar t...\n",
            "2. Machine Learning √© uma sub√°rea da IA que permite que computadores aprendam e melhorem\n",
            "    automatica...\n",
            "3. Deep Learning √© uma t√©cnica de machine learning baseada em redes neurais artificiais\n",
            "    com m√∫ltipl...\n"
          ]
        }
      ],
      "source": [
        "def busca_com_rerank(query, k=5, top_k=3):                                      # Define fun√ß√£o que busca e re-ranqueia documentos\n",
        "    \"\"\"Busca documentos com re-ranking usando Gemini\"\"\"                         # Docstring explicando a fun√ß√£o\n",
        "    print(f\"üîç Buscando documentos para: '{query}'\")                            # Mostra a query no console\n",
        "\n",
        "    try:                                                                        # Inicia bloco try/except para capturar erros\n",
        "        # Busca inicial (mais documentos)                                       # Coment√°rio: etapa de busca bruta\n",
        "        docs_iniciais = vectorstore.similarity_search(query, k=k)               # Recupera k documentos mais similares √† query\n",
        "        print(f\"üìÑ Documentos encontrados na busca inicial: {len(docs_iniciais)}\")  # Exibe quantidade de docs retornados\n",
        "\n",
        "        # Re-ranking                                                            # Coment√°rio: etapa de re-ranqueamento sem√¢ntico\n",
        "        docs_reranked = reranker.rerank(query, docs_iniciais, top_k=top_k)      # Reordena docs via embeddings Gemini e guarda top_k\n",
        "        print(f\"üéØ Documentos ap√≥s re-ranking: {len(docs_reranked)}\")            # Mostra quantos docs sobraram ap√≥s o re-rank\n",
        "\n",
        "        # Mostrar resultados                                                    # Coment√°rio: loop para exibir snippets dos docs\n",
        "        print(\"\\nüìä Documentos selecionados ap√≥s re-ranking:\")                  # Cabe√ßalho para a lista de docs finais\n",
        "        for i, doc in enumerate(docs_reranked, 1):                              # Itera sobre docs reranqueados enumerando a partir de 1\n",
        "            content = doc.page_content if hasattr(doc, 'page_content') else str(doc)  # Garante texto mesmo se n√£o for Document\n",
        "            print(f\"{i}. {content[:100]}...\")                                   # Exibe os primeiros 100 caracteres de cada doc\n",
        "\n",
        "        return docs_reranked                                                    # Retorna a lista final de documentos\n",
        "\n",
        "    except Exception as e:                                                      # Captura poss√≠veis exce√ß√µes\n",
        "        print(f\"‚ùå Erro na busca: {str(e)}\")                                     # Mostra mensagem de erro no console\n",
        "        return []                                                               # Retorna lista vazia em caso de falha\n",
        "\n",
        "# Teste do re-ranking                                                           # Coment√°rio: chamada de teste da fun√ß√£o\n",
        "docs_resultado = busca_com_rerank(\"machine learning algoritmos gemini\")         # Executa a fun√ß√£o com uma query de exemplo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLVEkNNPZ6tZ"
      },
      "source": [
        "## üéì 10. Conclus√£o\n",
        "\n",
        "Neste notebook, exploramos os principais conceitos de **Cadeias de Conversa√ß√£o Robusta** usando o **Google Gemini**:\n",
        "\n",
        "‚úÖ **ConversationalRetrievalChain**: Implementamos uma cadeia que mant√©m contexto conversacional com Gemini  \n",
        "‚úÖ **Gerenciamento de Mem√≥ria**: Configuramos mem√≥ria para manter hist√≥rico de intera√ß√µes  \n",
        "‚úÖ **Guardrails de Seguran√ßa**: Criamos filtros espec√≠ficos para proteger informa√ß√µes sens√≠veis  \n",
        "‚úÖ **Re-ranking com Gemini**: Implementamos re-ranking usando embeddings do Gemini  \n",
        "\n",
        "### üöÄ Vantagens do Google Gemini\n",
        "\n",
        "- **Multimodal**: Capacidade de processar texto, imagens e c√≥digo\n",
        "- **Custo-efetivo**: Pre√ßos competitivos comparado a outros modelos\n",
        "- **Integra√ß√£o Google**: F√°cil integra√ß√£o com outros servi√ßos Google\n",
        "- **Performance**: Excelente qualidade de respostas e racioc√≠nio\n",
        "\n",
        "### üîß Pr√≥ximos Passos\n",
        "\n",
        "1. **Experimente** com diferentes modelos Gemini (gemini-pro-vision para imagens)\n",
        "2. **Otimize** custos atrav√©s de an√°lise de tokens\n",
        "3. **Implemente** cache de respostas para reduzir chamadas √† API\n",
        "4. **Integre** com Google Cloud para produ√ß√£o\n",
        "5. **Explore** capacidades multimodais do Gemini\n",
        "\n",
        "### üìö Recursos Adicionais\n",
        "\n",
        "- [Google AI Studio](https://makersuite.google.com/)\n",
        "- [Documenta√ß√£o Gemini API](https://ai.google.dev/docs)\n",
        "- [LangChain + Gemini](https://python.langchain.com/docs/integrations/llms/google_ai)\n",
        "- [Pre√ßos do Gemini](https://ai.google.dev/pricing)\n",
        "\n",
        "---\n",
        "\n",
        "**Parab√©ns!** üéâ Voc√™ completou o notebook pr√°tico sobre Cadeias de Conversa√ß√£o Robusta com Google Gemini!"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CvlSr1gQsr40"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}